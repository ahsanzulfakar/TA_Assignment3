{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a38de2c",
   "metadata": {},
   "source": [
    "### Group members: Muhammad Ahsan Bin Zulfakar(SW01081423) & Arinn Danish Bin Abdullah (SW01081421)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570eed18",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ec5e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For text preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# For topic modeling\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import pandas as pd\n",
    "\n",
    "# Download NLTK Resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c9c9eb",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ca5b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('news_dataset.csv')\n",
    "documents = df['text'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7bdbc3",
   "metadata": {},
   "source": [
    "## Preprocess the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45c4433c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wondering', 'anyone', 'enlighten', 'car', 'saw', 'day', 'sport', 'car', 'looked', 'late', 'early', '70', 'called', 'bricklin', 'door', 'really', 'small', 'addition', 'front', 'bumper', 'separate', 'rest', 'body', 'anyone', 'tellme', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add custom stopwords: all lowercase and uppercase alphabetic characters\n",
    "custom_stop_words = {'could', 'would', 'like', 'know', 'get', 'one', 'use', 'using', 'door', \n",
    "                     'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', \n",
    "                     'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \n",
    "                     'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', \n",
    "                     'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'}\n",
    "\n",
    "stop_words.update(custom_stop_words)\n",
    "\n",
    "# Initialize a WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Ensure the input is a string\n",
    "    text = str(text)\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    # Filter out non-alphanumeric tokens and numbers\n",
    "    tokens = [token for token in tokens if token.isalnum() and not token.isnumeric()]\n",
    "    # Convert tokens to lowercase and remove stopwords\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "    # Lemmatize each token\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Preprocess each document in the list\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "# Print the first preprocessed document\n",
    "print(preprocessed_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f68c308",
   "metadata": {},
   "source": [
    "## Create document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03ee2d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gensim Dictionary object from the preprocessed documents\n",
    "dictionary = corpora.Dictionary(preprocessed_documents)\n",
    "# Filter out tokens that appear in less than 15 documents or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5)\n",
    "# Convert each preprocessed document into a bag-of-words representation using the dictionary\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2473e",
   "metadata": {},
   "source": [
    "## Run LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "395c898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LDA\n",
    "lda_model = LdaModel(corpus, num_topics=4, id2word=dictionary, passes=15) # Train an LDA model\n",
    "#on the corpus with 2 topics using Gensim's LdaModel class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d08edf",
   "metadata": {},
   "source": [
    "## Interpret Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ef80dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with Text and Topic:\n",
      "                                                    Text  Topic\n",
      "0      I was wondering if anyone out there could enli...      1\n",
      "1      I recently posted an article asking what kind ...      1\n",
      "2      \\nIt depends on your priorities.  A lot of peo...      1\n",
      "3      an excellent automatic can be found in the sub...      1\n",
      "4      : Ford and his automobile.  I need information...      1\n",
      "...                                                  ...    ...\n",
      "11309  Secrecy in Clipper Chip\\n\\nThe serial number o...      0\n",
      "11310  Hi !\\n\\nI am interested in the source of FEAL ...      0\n",
      "11311  The actual algorithm is classified, however, t...      3\n",
      "11312  \\n\\tThis appears to be generic calling upon th...      1\n",
      "11313  \\nProbably keep quiet and take it, lest they g...      1\n",
      "\n",
      "[11314 rows x 2 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# empty list to store dominant topic labels for each document\n",
    "text_labels = []\n",
    "\n",
    "# iterate over each processed document\n",
    "for i, doc in enumerate(preprocessed_documents):\n",
    " # for each document, convert to bag-of-words representation\n",
    " bow = dictionary.doc2bow(doc)\n",
    " # get list of topic probabilities\n",
    " topics = lda_model.get_document_topics(bow)\n",
    " # determine topic with highest probability\n",
    " dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    " # append to the list\n",
    " text_labels.append(dominant_topic)\n",
    "    \n",
    "# Create DataFrame\n",
    "df_result = pd.DataFrame({\"Text\": documents, \"Topic\": text_labels})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"Table with Text and Topic:\")\n",
    "print(df_result)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5d49d3",
   "metadata": {},
   "source": [
    "## Print top terms for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6562cbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms for Topic #0:\n",
      "['max', 'file', 'system', 'window', 'program', 'also', 'available', 'problem', 'version', 'drive']\n",
      "\n",
      "Top terms for Topic #1:\n",
      "['people', 'think', 'say', 'time', 'thing', 'god', 'even', 'see', 'make', 'well']\n",
      "\n",
      "Top terms for Topic #2:\n",
      "['game', 'db', 'team', 'year', 'player', 'play', 'new', 'season', 'first', 'league']\n",
      "\n",
      "Top terms for Topic #3:\n",
      "['government', 'key', 'people', 'armenian', 'state', 'president', 'encryption', 'law', 'right', 'public']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for topic_id in range(lda_model.num_topics):\n",
    " print(f\"Top terms for Topic #{topic_id}:\")\n",
    " top_terms = lda_model.show_topic(topic_id, topn=10)\n",
    " print([term[0] for term in top_terms])\n",
    " print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096a4b20",
   "metadata": {},
   "source": [
    "## Print the top terms for each topic with weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "96a4a1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Terms for Each Topic:\n",
      "Topic 0:\n",
      "- \"max\" (weight: 0.018)\n",
      "- \"file\" (weight: 0.011)\n",
      "- \"system\" (weight: 0.008)\n",
      "- \"window\" (weight: 0.007)\n",
      "- \"program\" (weight: 0.006)\n",
      "- \"also\" (weight: 0.005)\n",
      "- \"available\" (weight: 0.005)\n",
      "- \"problem\" (weight: 0.005)\n",
      "- \"version\" (weight: 0.005)\n",
      "- \"drive\" (weight: 0.005)\n",
      "\n",
      "Topic 1:\n",
      "- \"people\" (weight: 0.010)\n",
      "- \"think\" (weight: 0.009)\n",
      "- \"say\" (weight: 0.008)\n",
      "- \"time\" (weight: 0.006)\n",
      "- \"thing\" (weight: 0.006)\n",
      "- \"god\" (weight: 0.006)\n",
      "- \"even\" (weight: 0.006)\n",
      "- \"see\" (weight: 0.005)\n",
      "- \"make\" (weight: 0.005)\n",
      "- \"well\" (weight: 0.005)\n",
      "\n",
      "Topic 2:\n",
      "- \"game\" (weight: 0.016)\n",
      "- \"db\" (weight: 0.014)\n",
      "- \"team\" (weight: 0.013)\n",
      "- \"year\" (weight: 0.011)\n",
      "- \"player\" (weight: 0.008)\n",
      "- \"play\" (weight: 0.007)\n",
      "- \"new\" (weight: 0.006)\n",
      "- \"season\" (weight: 0.006)\n",
      "- \"first\" (weight: 0.005)\n",
      "- \"league\" (weight: 0.005)\n",
      "\n",
      "Topic 3:\n",
      "- \"government\" (weight: 0.009)\n",
      "- \"key\" (weight: 0.008)\n",
      "- \"people\" (weight: 0.007)\n",
      "- \"armenian\" (weight: 0.006)\n",
      "- \"state\" (weight: 0.006)\n",
      "- \"president\" (weight: 0.005)\n",
      "- \"encryption\" (weight: 0.005)\n",
      "- \"law\" (weight: 0.005)\n",
      "- \"right\" (weight: 0.004)\n",
      "- \"public\" (weight: 0.004)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Top Terms for Each Topic:\")\n",
    "\n",
    "for idx, topic in lda_model.print_topics():\n",
    " print(f\"Topic {idx}:\")\n",
    " terms = [term.strip() for term in topic.split(\"+\")]\n",
    " for term in terms:\n",
    "     weight, word = term.split(\"*\")\n",
    "     print(f\"- {word.strip()} (weight: {weight.strip()})\")\n",
    " print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec9205b",
   "metadata": {},
   "source": [
    "## Possible topic name\n",
    "\n",
    "### Topic 0: Computer System\n",
    "### Topic 1: Personal and general opinion\n",
    "### Topic 2: Sports and games\n",
    "### Topic 3: Politics and law\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c335691",
   "metadata": {},
   "source": [
    "### Evaluate the LDA Model using Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cdab0622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "71fff3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_documents, dictionary=dictionary, coherence='c_v')\n",
    "\n",
    "coherence_lda = coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bfa3283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Coherence Score (C_V): 0.5577\n"
     ]
    }
   ],
   "source": [
    "print(f'Topic Coherence Score (C_V): {coherence_lda:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674bf493",
   "metadata": {},
   "source": [
    "##### The Topic Coherence Score (C_V) of 0.5577 indicates a moderate level of coherence among the topics identified in the model. This score suggests that the terms within each topic are reasonably well-related, making the topics meaningful and interpretable. However, it also implies that there is room for improvement in the model's ability to group words into highly cohesive topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d933b4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
